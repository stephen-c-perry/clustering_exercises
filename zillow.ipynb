{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Exploring\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Visualizing\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# default pandas decimal number display format\n",
    "pd.options.display.float_format = '{:20,.2f}'.format\n",
    "\n",
    "import acquire\n",
    "import summarize as s\n",
    "import prepare as p\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zillow\n",
    "\n",
    "\n",
    "For the following, iterate through the steps you would take to create functions: Write the code to do the following in a jupyter notebook, test it, convert to functions, then create the file to house those functions.\n",
    "\n",
    "You will have a zillow.ipynb file and a helper file for each section in the pipeline.\n",
    "\n",
    "#### Acquire and Summarize\n",
    "\n",
    "1) Acquire data from the cloud database.\n",
    "\n",
    "You will want to end with a single dataframe. Include the logerror field and all other fields related to the properties that are available. You will end up using all the tables in the database.\n",
    "\n",
    "Be sure to do the correct join (inner, outer, etc.). We do not want to eliminate properties purely because they may have a null value for airconditioningtypeid. - Only include properties with a transaction in 2017, and include only the last transaction for each property (so no duplicate property ID's), along with zestimate error and date of transaction. (Hint: read the docs for the .duplicated method) - Only include properties that have a latitude and longitude value.\n",
    "\n",
    "2) Summarize your data (summary stats, info, dtypes, shape, distributions, value_counts, etc.)\n",
    "\n",
    "3) Write a function that takes in a dataframe of observations and attributes and returns a dataframe where each row is an atttribute name, the first column is the number of rows with missing values for that attribute, and the second column is percent of total rows that have missing values for that attribute. Run the function and document takeaways from this on how you want to handle missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acquiring data\n",
    "df = pd.read_csv('zillow_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/yogibexar/codeup-data-science/clustering_exercises/zillow.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yogibexar/codeup-data-science/clustering_exercises/zillow.ipynb#Y116sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m p\u001b[39m.\u001b[39;49mdf_described(df)\n",
      "File \u001b[0;32m~/codeup-data-science/clustering_exercises/prepare.py:5\u001b[0m, in \u001b[0;36mdf_described\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdf_described\u001b[39m(df):\n\u001b[1;32m      4\u001b[0m     df_transposed \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(df\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39miloc[:,\u001b[39m0\u001b[39m])\n\u001b[0;32m----> 5\u001b[0m     cols_to_drop \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49mshape[:,\u001b[39m0\u001b[39;49m]\n\u001b[1;32m      6\u001b[0m     df_transposed \u001b[39m=\u001b[39m df_transposed\u001b[39m.\u001b[39mdrop(columns \u001b[39m=\u001b[39m df_transposed[: , \u001b[39m-\u001b[39m cols_to_drop])\n\u001b[1;32m      7\u001b[0m     df_transposed[\u001b[39m'\u001b[39m\u001b[39m0\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39misnull()\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "p.df_described(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df.T.iloc[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.flip(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm no duplicates\n",
    "len(df)-len(df.drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure transactions are in 2017\n",
    "df.transactiondate.max(), df.transactiondate.min()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Summarize your data (summary stats, info, dtypes, shape, distributions, value_counts, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.summarize(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info = pd.DataFrame(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any(axis=1).sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3) Write a function that takes in a dataframe of observations and attributes and returns a dataframe where each row is an atttribute name, the first column is the number of rows with missing values for that attribute, and the second column is percent of total rows that have missing values for that attribute. Run the function and document takeaways from this on how you want to handle missing values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- each row attribute name\n",
    "- first column number of rows (of original df) with missing values\n",
    "- second column percent total columns with missing values for this attribute\n",
    "\n",
    "Not transforming original df, making a new df describing the original df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_described(df):\n",
    "    df_described = pd.DataFrame({\n",
    "                                    0: df.columns,\n",
    "                                    1: df.isnull().sum(axis=0),\n",
    "                                    2: df.isnull().any(axis=1).sum(), \n",
    "                                    3: df.shape[1] - df.count(axis=1),\n",
    "                                index = [df.columns]\n",
    "                                   } )\n",
    "    return df_described"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "      [[4, 7, 10],\n",
    "       [5, 8, 11],\n",
    "       [6, 9, 12]],\n",
    "      index=[1, 2, 3],\n",
    "      columns=['a', 'b', 'c'])\n",
    "\n",
    "df = pd.DataFrame(\n",
    "        [[df.columns],\n",
    "        [df.isnull().any(axis=1).sum()],\n",
    "        [],]\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transposed.drop(df_transposed.columns[0], axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transposed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def transform_df(df):\n",
    "    df = pd.DataFrame(df.T.iloc[:,2])\n",
    "    df.drop(df.columns[0], axis = 1, inplace = True)\n",
    "    df[0] = df.isnull().sum(axis=0)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transposed[0] = df.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transposed['1'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second column percent total columns with missing values for this attribute\n",
    "df_transposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "total columns.count - column[:1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare\n",
    "\n",
    "1) Remove any properties that are likely to be something other than single unit properties. (e.g. no duplexes, no land/lot, ...). There are multiple ways to estimate that a property is a single unit, and there is not a single \"right\" answer.\n",
    "\n",
    "2) Create a function that will drop rows or columns based on the percent of values that are missing: handle_missing_values(df, prop_required_column, prop_required_row).\n",
    "\n",
    "    - The input:\n",
    "        - A dataframe\n",
    "        - A number between 0 and 1 that represents the proportion, for each column, of rows with non-missing values required to keep the column. i.e. if prop_required_column = .6, then you are requiring a column to have at least 60% of values not-NA (no more than 40% missing).\n",
    "        - A number between 0 and 1 that represents the proportion, for each row, of columns/variables with non-missing values required to keep the row. For example, if prop_required_row = .75, then you are requiring a row to have at least 75% of variables with a non-missing value (no more that 25% missing).\n",
    "    - The output:\n",
    "        - The dataframe with the columns and rows dropped as indicated. Be sure to drop the columns prior to the rows in your function.\n",
    "    - hint:\n",
    "        - Look up the dropna documentation.\n",
    "        - You will want to compute a threshold from your input values (prop_required) and total number of rows or columns.\n",
    "3) Encapsulate your work inside of functions in a wrangle_zillow.py module."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mall Customers\n",
    "\n",
    "Notebook\n",
    "\n",
    "1) Acquire data from the customers table in the mall_customers database.\n",
    "2) Summarize the data (include distributions and descriptive statistics).\n",
    "3) Detect outliers using IQR.\n",
    "4) Split data into train, validate, and test.\n",
    "5) Encode categorical columns using a one hot encoder (pd.get_dummies).\n",
    "6) Handles missing values.\n",
    "7) Scaling\n",
    "\n",
    "Encapsulate your work in a wrangle_mall.py python module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
